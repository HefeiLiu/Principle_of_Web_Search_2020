{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from lxml import etree \n",
    "import pickle\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "import timeit\n",
    "import jieba\n",
    "\n",
    "class MySearcherC6V0:\n",
    "    \"\"\"\n",
    "    第五次课升级的搜索类版本：\n",
    "    1、增加初始化参数scale，用于倍增文档集\n",
    "    2、增加缓存机制，避免重复匹配相同关键词\n",
    "    3、增加线下缓存预填充机制，用猜测得到的用户查询词预填充\n",
    "    4、用文档分词得到的词表进行缓存预填充\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        self.docs = []\n",
    "        self.load_data()\n",
    "        self.docs *= scale\n",
    "        self.cache = {}\n",
    "        self.vocab = set()\n",
    "        self.build_cache()\n",
    "        \n",
    "    def build_cache(self):\n",
    "        for doc in self.docs:\n",
    "            for word in jieba.cut(\n",
    "                doc[1] +' ' + doc[2]\n",
    "            ):\n",
    "                r = self.search(word)\n",
    "                self.vocab.add(word)\n",
    "    \n",
    "    def load_data(self):\n",
    "        data_filename = 'news_list.dat'\n",
    "        if os.path.exists(data_filename):\n",
    "            with open(data_filename,'rb') as f:\n",
    "                self.docs += pickle.load(f)\n",
    "#                 self.docs = self.docs + pickle.load(f)\n",
    "        else:\n",
    "            url = 'http://news.163.com/special/0001386F/rank_tech.html'  \n",
    "            headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36 Edg/85.0.564.63'}\n",
    "            r = requests.get(url, headers=headers)  \n",
    "            sel = etree.HTML(r.text) \n",
    "            link_set = set()\n",
    "            news_list = []\n",
    "            count = 0\n",
    "            for item in sel.xpath('//td/a'):  \n",
    "                title = item.text\n",
    "                link = item.attrib['href']\n",
    "            #     print(link, title)\n",
    "                if link not in link_set:\n",
    "                    r = requests.get(link, headers=headers)  \n",
    "                    sel = etree.HTML(r.text)  \n",
    "                    text_block = sel.xpath('//div[@id=\"endText\"]') \n",
    "                #     print(''.join(text_block[0].itertext()))\n",
    "                    if text_block:\n",
    "                        content = ''.join(text_block[0].xpath('./p/text()'))\n",
    "                        title = sel.xpath('//h1/text()')[0]\n",
    "                        self.docs.append([link, title, content])\n",
    "                    link_set.add(link)\n",
    "                count += 1\n",
    "                if count % 15 == 0:\n",
    "                    print(count, 'processed.')\n",
    "            with open(data_filename,'wb') as f:\n",
    "                pickle.dump(self.docs, f)\n",
    "    \n",
    "    def search(self, keyword):\n",
    "        keyword_l = keyword.lower()\n",
    "        if keyword_l in self.cache:\n",
    "            sorted_result = self.cache[keyword_l] \n",
    "        else:\n",
    "            count = 0\n",
    "            sorted_result = []\n",
    "            for item in self.docs:\n",
    "                if keyword_l in (item[1] + item[2]).lower():\n",
    "            #         count += 1\n",
    "            #         print(count, highlight(title, keyword))\n",
    "                    sorted_result.append([count, self.score(item, keyword)])\n",
    "                count += 1\n",
    "            sorted_result.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.cache[keyword_l] = sorted_result\n",
    "        return sorted_result\n",
    "    \n",
    "    def highlight(self, text, keyword):\n",
    "        idx = text.lower().find(keyword.lower())\n",
    "        result = text\n",
    "        if idx >= 0:\n",
    "            ori_word = text[idx:idx+(len(keyword))]\n",
    "            result = text.replace(ori_word, '<span style=\"color:red\";>{}</span>'.format(ori_word))\n",
    "        return result\n",
    "    \n",
    "    def score(self, item, keyword):\n",
    "        return (item[1].lower().count(keyword.lower()) * 5 \n",
    "          + item[2].lower().count(keyword.lower()) * 3)\n",
    "    \n",
    "    def render_search_result(self, keyword):\n",
    "        count = 0\n",
    "        for item in self.search(keyword):\n",
    "            count += 1\n",
    "        #     print(count, '[{}] {}'.format(item[1], \n",
    "        #         highlight(news_list[item[0]][1], keyword)))\n",
    "            display(HTML('{} [{}] {}'.format(count, item[1], \n",
    "                self.highlight(self.docs[item[0]][1], keyword))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcher_1x = MySearcherC6V0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         11591540 function calls in 138.802 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "  6071604   93.981    0.000   93.981    0.000 {method 'lower' of 'str' objects}\n",
       "   215165   31.666    0.000  127.703    0.001 <ipython-input-8-9c1e8c6ac724>:67(search)\n",
       "    31598    2.139    0.000    2.319    0.000 __init__.py:180(get_DAG)\n",
       "   805368    1.407    0.000    1.992    0.000 __init__.py:177(<genexpr>)\n",
       "   308868    0.911    0.000    0.911    0.000 {method 'count' of 'str' objects}\n",
       "   203750    0.904    0.000    9.289    0.000 __init__.py:249(__cut_DAG)\n",
       "    31598    0.849    0.000    3.617    0.000 __init__.py:172(calc)\n",
       "   425487    0.837    0.000    2.909    0.000 {built-in method builtins.max}\n",
       "    18019    0.790    0.000    1.507    0.000 __init__.py:37(viterbi)\n",
       "   154434    0.776    0.000    5.769    0.000 <ipython-input-8-9c1e8c6ac724>:92(score)\n",
       "   215454    0.643    0.000   10.262    0.000 __init__.py:289(cut)\n",
       "        1    0.610    0.610  138.770  138.770 <ipython-input-8-9c1e8c6ac724>:25(build_cache)\n",
       "   786864    0.478    0.000    0.478    0.000 {method 'get' of 'dict' objects}\n",
       "   503502    0.417    0.000    0.417    0.000 {built-in method math.log}\n",
       "    61169    0.342    0.000    2.394    0.000 __init__.py:85(cut)\n",
       "   173969    0.297    0.000    0.297    0.000 {method 'match' of 're.Pattern' objects}\n",
       "    90775    0.282    0.000    0.282    0.000 {method 'split' of 're.Pattern' objects}\n",
       "    74004    0.258    0.000    0.300    0.000 __init__.py:49(<listcomp>)\n",
       "    47013    0.236    0.000    1.751    0.000 __init__.py:59(__cut)\n",
       "    18127    0.232    0.000    0.286    0.000 {method 'sort' of 'list' objects}\n",
       "   644839    0.228    0.000    0.228    0.000 {method 'append' of 'list' objects}\n",
       "   215165    0.194    0.000    0.194    0.000 {method 'add' of 'set' objects}\n",
       "   213181    0.087    0.000    0.087    0.000 {built-in method builtins.len}\n",
       "    54057    0.069    0.000    0.080    0.000 __init__.py:54(<genexpr>)\n",
       "   154434    0.054    0.000    0.054    0.000 <ipython-input-8-9c1e8c6ac724>:80(<lambda>)\n",
       "    20744    0.033    0.000    0.052    0.000 _compat.py:76(strdecode)\n",
       "    31598    0.030    0.000    0.030    0.000 __init__.py:168(check_initialized)\n",
       "    20744    0.019    0.000    0.019    0.000 {built-in method builtins.isinstance}\n",
       "        1    0.019    0.019  138.802  138.802 <string>:1(<module>)\n",
       "        1    0.012    0.012    0.012    0.012 {built-in method _pickle.load}\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method nt.stat}\n",
       "        1    0.000    0.000  138.802  138.802 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.013    0.013 <ipython-input-8-9c1e8c6ac724>:33(load_data)\n",
       "        1    0.000    0.000  138.783  138.783 <ipython-input-8-9c1e8c6ac724>:17(__init__)\n",
       "        1    0.000    0.000    0.001    0.001 genericpath.py:16(exists)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%prun searcher_1x = MySearcherC6V0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V1(MySearcherC6V0):\n",
    "    \"\"\"\n",
    "    避免重复查询相同词\n",
    "    \"\"\"\n",
    "    def build_cache(self):\n",
    "        word_set = set()\n",
    "        for doc in self.docs:\n",
    "            for word in jieba.cut(\n",
    "                doc[1] +' ' + doc[2]\n",
    "            ):\n",
    "                if word not in word_set:\n",
    "                    r = self.search(word)\n",
    "                    self.vocab.add(word)\n",
    "                    word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcher_1x = MySearcherC6V1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V2(MySearcherC6V1):\n",
    "    \"\"\"\n",
    "    尽量减少lower()的运行次数\n",
    "    \"\"\"\n",
    "    def __init__(self, scale=1):\n",
    "        self.docs = []\n",
    "        self.load_data()\n",
    "        self.docs *= scale\n",
    "        self.cache = {}\n",
    "        self.vocab = set()\n",
    "        self.lower_preprocess()\n",
    "        self.build_cache()\n",
    "        self.simple_test()\n",
    "    \n",
    "    def build_cache(self):\n",
    "        word_set = set()\n",
    "        for doc in self.docs:\n",
    "            for word in jieba.cut(\n",
    "                doc[1] +' ' + doc[2]\n",
    "            ):\n",
    "                if word not in word_set:\n",
    "                    r = self.search(word)\n",
    "                    self.vocab.add(word)\n",
    "                    word_set.add(word)\n",
    "                    \n",
    "    def lower_preprocess(self):\n",
    "        for doc_id in range(len(self.docs)):\n",
    "            self.docs[doc_id].append(\n",
    "                (self.docs[doc_id][1] \n",
    "                 + ' ' \n",
    "                 + self.docs[doc_id][2]).lower()\n",
    "            )\n",
    "        \n",
    "    def search(self, keyword):\n",
    "        keyword_l = keyword.lower()\n",
    "        if keyword_l in self.cache:\n",
    "            sorted_result = self.cache[keyword_l] \n",
    "        else:\n",
    "            count = 0\n",
    "            sorted_result = []\n",
    "            for item in self.docs:\n",
    "                if keyword_l in item[3]:\n",
    "            #         count += 1\n",
    "            #         print(count, highlight(title, keyword))\n",
    "                    sorted_result.append([count, self.score(item, keyword)])\n",
    "                count += 1\n",
    "            sorted_result.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.cache[keyword_l] = sorted_result\n",
    "        return sorted_result\n",
    "    \n",
    "    def simple_test(self):\n",
    "        assert(len(self.search('tiktok')) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherv2_1x = MySearcherC6V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V3(MySearcherC6V2):\n",
    "    \"\"\"\n",
    "    用文档刷词构建缓存\n",
    "    \"\"\"\n",
    "    def build_cache(self):\n",
    "        doc_id = 0\n",
    "        for doc in self.docs:\n",
    "            doc_word_set = set()\n",
    "            for word in jieba.cut(\n",
    "                doc[3]\n",
    "            ):\n",
    "                if word not in doc_word_set:\n",
    "                    result_item = [doc_id, self.score(doc, word)]\n",
    "                    if word not in self.cache:\n",
    "                        self.cache[word] = [result_item]\n",
    "                    else:\n",
    "                        self.cache[word].append(result_item)\n",
    "                    self.vocab.add(word)\n",
    "                    doc_word_set.add(word)\n",
    "            doc_id += 1\n",
    "        \n",
    "        for word in self.cache:\n",
    "            self.cache[word].sort(key=lambda x: x[1], reverse=True)\n",
    "                    \n",
    "    def __init__(self, scale=1):\n",
    "        self.docs = []\n",
    "        self.load_data()\n",
    "        self.docs *= scale\n",
    "        self.cache = {}\n",
    "        self.vocab = set()\n",
    "        self.lower_preprocess()\n",
    "        self.build_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherv3_1x = MySearcherC6V3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1 [41] <span style=\"color:red\";>B站</span>CEO陈睿：5G时代视频将是绝对的主流"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "2 [38] 腾讯系虎牙斗鱼终于合并，但快手抖音<span style=\"color:red\";>B站</span>已杀来"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3 [3] 乐元素荣列2020北京民企文化产业百强榜单前三"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherv3_1x.render_search_result('B站')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySearcherC6V4(MySearcherC6V3):\n",
    "    \"\"\"\n",
    "    去掉search里的文档扫描过程\n",
    "    \"\"\"\n",
    "    def search(self, keyword):\n",
    "        keyword_l = keyword.lower()\n",
    "        if keyword_l in self.cache:\n",
    "            sorted_result = self.cache[keyword_l] \n",
    "        else:\n",
    "            sorted_result = []\n",
    "        return sorted_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "searcherv4_1x = MySearcherC6V4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "searcherv4_1x.search('b站')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B 站 CEO 陈睿 ： 5G 时代 视频 将 是 绝对 的 主流'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(jieba.cut('B站CEO陈睿：5G时代视频将是绝对的主流'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
